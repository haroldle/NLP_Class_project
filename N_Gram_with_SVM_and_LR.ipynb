{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f52c01f",
   "metadata": {},
   "source": [
    "# IMPORTING LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1d2cd039",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib\n",
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05623ff9",
   "metadata": {},
   "source": [
    "# READ THE PREPROCESSED DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89a75ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('preprocessed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cdfaf9",
   "metadata": {},
   "source": [
    "# VISUALIZE FIRST ROW TO UNDERSTAND THE DATA FORMAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fdfb24d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>question1_non_ascii</th>\n",
       "      <th>question2_non_ascii</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  id  qid1  qid2  \\\n",
       "0           0   0     1     2   \n",
       "\n",
       "                                           question1  \\\n",
       "0  What is the step by step guide to invest in sh...   \n",
       "\n",
       "                                           question2  is_duplicate  \\\n",
       "0  What is the step by step guide to invest in sh...             0   \n",
       "\n",
       "                                 question1_non_ascii  \\\n",
       "0  What is the step by step guide to invest in sh...   \n",
       "\n",
       "                                 question2_non_ascii  \n",
       "0  What is the step by step guide to invest in sh...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c21b2c8",
   "metadata": {},
   "source": [
    "# GET THE LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e175de9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df['is_duplicate']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed1fd14",
   "metadata": {},
   "source": [
    "# MERGE QUESTION 1 AND 2 DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3db7b2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df.loc[:,['question1_non_ascii','question2_non_ascii']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66d80ad",
   "metadata": {},
   "source": [
    "# CREATE TRAIN/TEST SETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c3ff71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(corpus, labels, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c518ef17",
   "metadata": {},
   "source": [
    "# CREATE TRAIN CORPUS USING TRAIN SET TO CREATE BOW (N GRAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7a59e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = pd.concat((X_train['question1_non_ascii'], X_train['question2_non_ascii']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2600eba6",
   "metadata": {},
   "source": [
    "# CREATE BOW (N GRAMS) (UNI, BI, TRIGRAM) FOR EACH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ead31e63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thanhle/.virtualenvs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:524: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# CREATE BOW OBJECT FOR UNIGRAM\n",
    "vect = CountVectorizer(analyzer='word', ngram_range=(1, 1), tokenizer = word_tokenize)\n",
    "# CREATE BOW OBJECT FOR BIGRAM\n",
    "vect_2 = CountVectorizer(analyzer='word', ngram_range=(1, 2), tokenizer = word_tokenize)\n",
    "# CREATE BOW OBJECT FOR TRIGRAM\n",
    "vect_3 = CountVectorizer(analyzer='word', ngram_range=(1, 3), tokenizer = word_tokenize)\n",
    "\n",
    "# CREATE BOW FOR UNIGRAM\n",
    "vect.fit(train_corpus)\n",
    "# CREATE BOW FOR BIGRAM\n",
    "vect_2.fit(train_corpus)\n",
    "# CREATE BOW FOR TRIGRAM\n",
    "vect_3.fit(train_corpus)\n",
    "\n",
    "# TRANSFORM QUESTION 1 AND 2 INTO BOW (UNIGRAM) REPRESENTATION. (TRAINSET)\n",
    "train = vect.transform(X_train['question1_non_ascii'].values)\n",
    "train_2 = vect.transform(X_train['question2_non_ascii'].values)\n",
    "# CREATE UNIGRAM TRAINSET\n",
    "Unigram_train = scipy.sparse.hstack((train, train_2))\n",
    "\n",
    "# TRANSFORM QUESTION 1 AND 2 INTO BOW (UNIGRAM) REPRESENTATION. (TESTSET)\n",
    "test_1 = vect.transform(X_test['question1_non_ascii'].values)\n",
    "test_2 = vect.transform(X_test['question2_non_ascii'].values)\n",
    "# CREATE UNIGRAM TESTSET\n",
    "Unigram_test = scipy.sparse.hstack((test_1, test_2))\n",
    "\n",
    "# TRANSFORM QUESTION 1 AND 2 INTO BOW (BIGRAM) REPRESENTATION. (TRAINSET)\n",
    "train = vect_2.transform(X_train['question1_non_ascii'].values)\n",
    "train_2 = vect_2.transform(X_train['question2_non_ascii'].values)\n",
    "# CREATE BIGRAM TRAINSET\n",
    "Bigram_train = scipy.sparse.hstack((train, train_2))\n",
    "\n",
    "# TRANSFORM QUESTION 1 AND 2 INTO BOW (BIGRAM) REPRESENTATION. (TESTSET)\n",
    "test_1 = vect_2.transform(X_test['question1_non_ascii'].values)\n",
    "test_2 = vect_2.transform(X_test['question2_non_ascii'].values)\n",
    "# CREATE BIGRAM TESTSET\n",
    "Bigram_test = scipy.sparse.hstack((test_1, test_2))\n",
    "\n",
    "# TRANSFORM QUESTION 1 AND 2 INTO BOW (TRIGRAM) REPRESENTATION. (TRAINSET)\n",
    "train = vect_3.transform(X_train['question1_non_ascii'].values)\n",
    "train_2 = vect_3.transform(X_train['question2_non_ascii'].values)\n",
    "# CREATE TRIGRAM TRAINSET\n",
    "Trigram_train = scipy.sparse.hstack((train, train_2))\n",
    "\n",
    "# TRANSFORM QUESTION 1 AND 2 INTO BOW (TRIGRAM) REPRESENTATION. (TESTSET)\n",
    "test_1 = vect_3.transform(X_test['question1_non_ascii'].values)\n",
    "test_2 = vect_3.transform(X_test['question2_non_ascii'].values)\n",
    "# CREATE TRIGRAM TESTSET\n",
    "Trigram_test = scipy.sparse.hstack((test_1, test_2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf68ab82",
   "metadata": {},
   "source": [
    "# SVM SECTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "deb87c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE VALIDATION METHOD THAT SPLIT TRAIN SET INTO TRAIN/VAL SET INTO (70/20 RELATIVE TO GLOBALLY DATASET)\n",
    "SSP = StratifiedShuffleSplit(n_splits=3, test_size=0.22, random_state=42)\n",
    "# LABELS' NAMES\n",
    "target_names = ['Not duplicate', 'duplicate']\n",
    "# SET TUNE PARAMETER FOR SVM\n",
    "parameters = {'max_iter':[10000]}\n",
    "# SET TUNE PARAMETER FOR LOGISTIC REGRESSION\n",
    "parameter_LR = {'max_iter':[20]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4400acf8",
   "metadata": {},
   "source": [
    "## SVM FOR UNIGRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c3ab739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7285897847735304\n"
     ]
    }
   ],
   "source": [
    "clf = LinearSVC(random_state=42, tol=1e-3, max_iter=10000)\n",
    "clf = GridSearchCV(clf, parameters, cv=SSP)\n",
    "clf.fit(Unigram_train, y_train)\n",
    "cv_results = clf.best_score_\n",
    "clf = clf.best_estimator_\n",
    "print(cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15c3e257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "Not duplicate       0.80      0.78      0.79     76609\n",
      "    duplicate       0.64      0.67      0.65     44678\n",
      "\n",
      "     accuracy                           0.74    121287\n",
      "    macro avg       0.72      0.72      0.72    121287\n",
      " weighted avg       0.74      0.74      0.74    121287\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, clf.predict(Unigram_test), target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa691d2",
   "metadata": {},
   "source": [
    "## SVM FOR BIGRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f86f7e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7609594174965199\n"
     ]
    }
   ],
   "source": [
    "clf_2 = LinearSVC(random_state=42, tol=1e-3, max_iter=10000)\n",
    "clf_2 = GridSearchCV(clf_2, parameters, cv=SSP)\n",
    "clf_2.fit(Bigram_train, y_train)\n",
    "cv_results = clf_2.best_score_\n",
    "clf_2 = clf_2.best_estimator_\n",
    "print(cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4009272e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "Not duplicate       0.82      0.81      0.82     76609\n",
      "    duplicate       0.68      0.70      0.69     44678\n",
      "\n",
      "     accuracy                           0.77    121287\n",
      "    macro avg       0.75      0.75      0.75    121287\n",
      " weighted avg       0.77      0.77      0.77    121287\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, clf_2.predict(Bigram_test), target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0a7155",
   "metadata": {},
   "source": [
    "## SVM FOR TRIGRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0150528d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7828032979976443\n"
     ]
    }
   ],
   "source": [
    "clf_3 = LinearSVC(random_state=42, tol=1e-3, max_iter=10000)\n",
    "clf_3 = GridSearchCV(clf_3, parameters, cv=SSP)\n",
    "clf_3.fit(Trigram_train, y_train)\n",
    "cv_results = clf_3.best_score_\n",
    "clf_3 = clf_3.best_estimator_\n",
    "print(cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cd0726a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "Not duplicate       0.83      0.85      0.84     76609\n",
      "    duplicate       0.72      0.70      0.71     44678\n",
      "\n",
      "     accuracy                           0.79    121287\n",
      "    macro avg       0.78      0.77      0.77    121287\n",
      " weighted avg       0.79      0.79      0.79    121287\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, clf_3.predict(Trigram_test), target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6913cd9b",
   "metadata": {},
   "source": [
    "## EXPORT SVM MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7c1292f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SVM_Trigram.joblib']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(clf, 'SVM_Unigram.joblib')\n",
    "joblib.dump(clf_2, 'SVM_Bigram.joblib')\n",
    "joblib.dump(clf_3, 'SVM_Trigram.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a8c490",
   "metadata": {},
   "source": [
    "# LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9b9932",
   "metadata": {},
   "source": [
    "## LR FOR UNIGRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9fd1cdfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thanhle/.virtualenvs/nlp/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:705: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/thanhle/.virtualenvs/nlp/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:705: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/thanhle/.virtualenvs/nlp/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:705: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7478102580576079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thanhle/.virtualenvs/nlp/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:705: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "clf_LR = SGDClassifier(penalty='l2',\n",
    "                    loss = 'log_loss',\n",
    "                    learning_rate='optimal',\n",
    "                    max_iter=20,\n",
    "                    alpha = 0.00001,\n",
    "                    tol=1e-3,\n",
    "                    n_jobs = 10)\n",
    "\n",
    "clf_LR = GridSearchCV(clf_LR, parameter_LR, cv=SSP)\n",
    "clf_LR.fit(Unigram_train, y_train)\n",
    "cv_results = clf_LR.best_score_\n",
    "clf_LR = clf_LR.best_estimator_\n",
    "print(cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b199413c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "Not duplicate       0.79      0.82      0.80     76609\n",
      "    duplicate       0.67      0.62      0.64     44678\n",
      "\n",
      "     accuracy                           0.75    121287\n",
      "    macro avg       0.73      0.72      0.72    121287\n",
      " weighted avg       0.74      0.75      0.74    121287\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, clf_LR.predict(Unigram_test), target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a32bad",
   "metadata": {},
   "source": [
    "## LR FOR BIGRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "be038414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7835689045936395\n"
     ]
    }
   ],
   "source": [
    "clf_LR_2 = SGDClassifier(penalty='l2',\n",
    "                    loss = 'log_loss',\n",
    "                    learning_rate='optimal',\n",
    "                    max_iter=20,\n",
    "                    alpha = 0.00001,\n",
    "                    tol=1e-3,\n",
    "                    n_jobs = 10)\n",
    "\n",
    "clf_LR_2 = GridSearchCV(clf_LR_2, parameter_LR, cv=SSP)\n",
    "clf_LR_2.fit(Bigram_train, y_train)\n",
    "cv_results = clf_LR_2.best_score_\n",
    "clf_LR_2 = clf_LR_2.best_estimator_\n",
    "print(cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a42ee324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "Not duplicate       0.83      0.84      0.83     76609\n",
      "    duplicate       0.72      0.71      0.71     44678\n",
      "\n",
      "     accuracy                           0.79    121287\n",
      "    macro avg       0.77      0.77      0.77    121287\n",
      " weighted avg       0.79      0.79      0.79    121287\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, clf_LR_2.predict(Bigram_test), target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b012c0",
   "metadata": {},
   "source": [
    "## LR FOR TRIGRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "db0c59b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7978209658421672\n"
     ]
    }
   ],
   "source": [
    "clf_LR_3 = SGDClassifier(penalty='l2',\n",
    "                    loss = 'log_loss',\n",
    "                    learning_rate='optimal',\n",
    "                    max_iter=20,\n",
    "                    alpha = 0.00001,\n",
    "                    tol=1e-3,\n",
    "                    n_jobs = 10)\n",
    "\n",
    "clf_LR_3 = GridSearchCV(clf_LR_3, parameter_LR, cv=SSP)\n",
    "clf_LR_3.fit(Trigram_train, y_train)\n",
    "cv_results = clf_LR_3.best_score_\n",
    "clf_LR_3 = clf_LR_3.best_estimator_\n",
    "print(cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "41178d0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "Not duplicate       0.82      0.88      0.85     76609\n",
      "    duplicate       0.77      0.67      0.72     44678\n",
      "\n",
      "     accuracy                           0.80    121287\n",
      "    macro avg       0.79      0.78      0.78    121287\n",
      " weighted avg       0.80      0.80      0.80    121287\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, clf_LR_3.predict(Trigram_test), target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ed5af2",
   "metadata": {},
   "source": [
    "## EXPORT LR MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2c2f15ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LR_Trigram.joblib']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(clf_LR, 'LR_Unigram.joblib')\n",
    "joblib.dump(clf_LR_2, 'LR_Bigram.joblib')\n",
    "joblib.dump(clf_LR_3, 'LR_Trigram.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
